# Copyright (c) OpenMMLab. All rights reserved.
import torch
import torch.nn as nn
import torch.nn.functional as F
from mmcv.cnn import (ConvModule, bias_init_with_prob, build_plugin_layer,
                      normal_init)
from mmcv.runner import BaseModule

from mmdet.core import build_assigner, build_sampler, multi_apply
from mmdet.models.builder import HEADS, build_loss


@HEADS.register_module()
class KernelRPNHead(BaseModule):
    """KernelRPNHead.

    See `K-Net: Towards Unified Image Segmentation
    <https://arxiv.org/pdf/2106.14855>`_ for more details.

    Args:
        in_channels (int): Number of channels in the input feature map.
        out_channels (int): Number of channels in the output feature map.
        num_proposals (int): Number of proposals generated by RPN.
        num_things_classes (int): Number of things. Defaults to 80.
        num_stuff_classes (int): Number of stuff. Defaults to 53.
        ignore_label (int): Class indice to be ignored. Defaults to 255.
        num_cls_fcs (int): Number of fully connected layers
            in the classification branch. Defaults to 1.
        num_seg_convs (int): Number of convolution layers in the
            segmentation branch. Defaults to 1.
        num_loc_convs (int): Number of convolution layers in the
            location branch. Defaults to 1.
        localization_fpn_cfg (:obj:`mmcv.Config`): Config of semantic FPN.
        conv_kernel_size (int): Kernel size. Defaults to 1.
        norm_cfg (:obj:`mmcv.Config`): Norm config.
            Defaults to dict(type='GN', num_groups=32).
        feat_scale_factor (int): feature map scale factor.
            Defaults to 2.
        loss_rank (:obj:`mmcv.Config`): Rank loss config.
            Defaults to dict( type='CrossEntropyLoss', ...).
        loss_mask (:obj:`mmcv.Config`): Mask loss config.
            Defaults to dict( type='CrossEntropyLoss', ...).
        loss_dice (:obj:`mmcv.Config`): Dice loss config.
            Defaults to dict(type='DiceLoss', ...).
        loss_seg (:obj:`mmcv.Config`): Segmentation loss config.
            Defaults to dict( type='FocalLoss', ...).
        train_cfg (:obj:`mmcv.Config`, optional): Training config.
            Defaults to None.
        test_cfg (:obj:`mmcv.Config`, optional): Testing config.
            Defaults to None.
        init_cfg (dict or list[dict], optional): Initialization config dict.
            Defaults to None.
    """

    def __init__(self,
                 in_channels,
                 out_channels,
                 num_proposals,
                 num_things_classes=80,
                 num_stuff_classes=53,
                 ignore_label=255,
                 num_cls_fcs=1,
                 num_seg_convs=1,
                 num_loc_convs=1,
                 localization_fpn_cfg=dict(
                     type='SemanticFPN',
                     in_channels=256,
                     feat_channels=256,
                     out_channels=256,
                     start_level=0,
                     end_level=3,
                     output_level=1,
                     positional_encoding_level=3,
                     positional_encoding_cfg=dict(
                         type='SinePositionalEncoding',
                         num_feats=128,
                         normalize=True),
                     add_aux_conv=True,
                     out_act_cfg=dict(type='ReLU'),
                     conv_cfg=None,
                     norm_cfg=dict(
                         type='GN', num_groups=32, requires_grad=True)),
                 conv_kernel_size=1,
                 norm_cfg=dict(type='GN', num_groups=32),
                 feat_scale_factor=2,
                 loss_rank=dict(
                     type='CrossEntropyLoss',
                     use_sigmoid=False,
                     loss_weight=0.1),
                 loss_mask=dict(
                     type='CrossEntropyLoss',
                     use_sigmoid=True,
                     loss_weight=1.0),
                 loss_dice=dict(type='DiceLoss', loss_weight=4.0),
                 loss_seg=dict(
                     type='FocalLoss',
                     use_sigmoid=True,
                     gamma=2.0,
                     alpha=0.25,
                     loss_weight=1.0),
                 train_cfg=None,
                 test_cfg=None,
                 init_cfg=None):
        super().__init__(init_cfg=init_cfg)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_proposals = num_proposals

        self.num_things_classes = num_things_classes
        self.num_stuff_classes = num_stuff_classes
        self.num_classes = num_things_classes + num_stuff_classes
        self.ignore_label = ignore_label
        self.cat_stuff_mask = num_stuff_classes > 0

        self.num_cls_fcs = num_cls_fcs
        self.num_seg_convs = num_seg_convs
        self.num_loc_convs = num_loc_convs

        self.localization_fpn_cfg = localization_fpn_cfg
        self.conv_kernel_size = conv_kernel_size
        self.norm_cfg = norm_cfg

        self.feat_scale_factor = feat_scale_factor

        self.loss_rank = build_loss(loss_rank) if loss_rank else None
        self.loss_mask = build_loss(loss_mask)
        self.loss_dice = build_loss(loss_dice)
        self.loss_seg = build_loss(loss_seg)

        self.test_cfg = test_cfg
        self.train_cfg = train_cfg
        if self.train_cfg:
            self.assigner = build_assigner(self.train_cfg.assigner)
            sampler_cfg = dict(type='MaskPseudoSampler')
            self.sampler = build_sampler(sampler_cfg, context=self)

        self._init_layers()

    def _init_layers(self):
        """Initialize layers."""
        self.localization_fpn = build_plugin_layer(
            self.localization_fpn_cfg)[1]

        self.init_kernels = nn.Conv2d(
            self.out_channels,
            self.num_proposals,
            kernel_size=self.conv_kernel_size,
            padding=int(self.conv_kernel_size // 2),
            stride=1,
            bias=False)

        self.conv_seg = nn.Conv2d(
            self.out_channels,
            self.num_classes if self.loss_seg.use_sigmoid else
            (self.num_classes + 1),
            kernel_size=1)

        self.loc_convs = nn.Sequential()
        for _ in range(self.num_loc_convs):
            self.loc_convs.add_module(
                str(len(self.loc_convs)),
                ConvModule(
                    self.in_channels,
                    self.out_channels,
                    kernel_size=1,
                    padding=0,
                    stride=1,
                    norm_cfg=self.norm_cfg))

        self.seg_convs = nn.Sequential()
        for _ in range(self.num_seg_convs):
            self.seg_convs.add_module(
                str(len(self.seg_convs)),
                ConvModule(
                    self.in_channels,
                    self.out_channels,
                    kernel_size=1,
                    padding=0,
                    stride=1,
                    norm_cfg=self.norm_cfg))

    def init_weights(self):
        """Initialize weights."""
        self.localization_fpn.init_weights()

        for conv in [self.loc_convs, self.seg_convs]:
            for m in conv.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01)

        bias_seg = bias_init_with_prob(0.01)
        if self.loss_seg.use_sigmoid:
            normal_init(self.conv_seg, std=0.01, bias=bias_seg)
        else:
            normal_init(self.conv_seg, mean=0, std=0.01)

        normal_init(self.init_kernels, mean=0, std=1)

    def _cat_stuff_mask(self, mask_preds, seg_preds, proposal_feats):
        """Concatenate stuff masks and things masks togather, concatenate stuff
        kernel and things kernel togather."""
        mask_preds = torch.cat(
            [mask_preds, seg_preds[:, self.num_things_classes:]], dim=1)
        stuff_kernels = self.conv_seg.weight[self.num_things_classes:].clone()
        # shape (c_out, c_in, k, k) -> (batch_size, c_out, c_in, k, k)
        stuff_kernels = stuff_kernels[None].expand((mask_preds.shape[0], ) +
                                                   stuff_kernels.shape)
        proposal_feats = torch.cat([proposal_feats, stuff_kernels], dim=1)
        return mask_preds, proposal_feats

    def forward(self, x, img_metas):
        """
        Args:
            x (tuple[torch.Tensor]): Multi-level features from the
                upstream network, each is a 4D-tensor.
            img_metas (list[dict]): List of image information.

        Returns:
            tuple[Tensor]: a tuple contains the following tensors:

            - x_feats (Tensor): Feature maps, shape (batch_size, c, h, w).
            - proposal_feats (Tensor): Proposal feats, shape (batch_size,
              num_proposals, c, kernel_size, kernel_size).
            - mask_preds (Tensor): Mask logits, shape (batch_size,
              num_proposals, h, w).
            - seg_preds (Tensor): Segmentation logits, shape
              (batch_size, num_stuff_classes, h, w).
        """
        num_imgs = len(img_metas)

        loc_feats, sem_feats = self.localization_fpn(x)

        loc_feats = self.loc_convs(loc_feats)
        sem_feats = self.seg_convs(sem_feats)

        mask_preds = self.init_kernels(loc_feats)
        seg_preds = self.conv_seg(sem_feats)

        proposal_feats = self.init_kernels.weight.clone()
        # shape (c_out, c_in, kernel_size, kernel_size) ->
        # (batch_size, c_out, c_in, kernel_size, kernel_size)
        proposal_feats = proposal_feats[None].expand(num_imgs,
                                                     *proposal_feats.shape)
        if sem_feats is not None:
            x_feats = sem_feats + loc_feats
        else:
            x_feats = loc_feats

        sigmoid_masks = mask_preds.sigmoid()
        nonzeros_inds = sigmoid_masks > 0.5
        sigmoid_masks = nonzeros_inds.float()
        obj_feats = torch.einsum('bnhw,bchw->bnc', sigmoid_masks, x_feats)

        proposal_feats = proposal_feats + obj_feats.view(
            num_imgs, self.num_proposals, self.out_channels, 1, 1)

        return x_feats, proposal_feats, mask_preds, seg_preds

    def _get_targets_single(self, pos_inds, pos_mask, neg_mask, pos_gt_mask,
                            pos_gt_labels, gt_sem_seg, gt_sem_cls):
        """
        Args:
            pos_inds (Tensor): Indice of positive samples.
            pos_mask (Tensor): Positive masks.
            neg_mask (Tensor): Negative masks.
            pos_gt_mask (Tensor): Positive ground truth instance masks.
            pos_gt_labels (Tensor): Positive groud truth instance class
                indices.
            gt_sem_seg (Tensor): Ground truth stuff masks.
            gt_sem_cls (Tensor): Ground truth stuff class indices.

        Returns:
            tuple:

            - labels (Tensor): shape (num_proposals,).
            - mask_targets (Tensor): shape (num_proposals, h, w).
            - seg_targets (Tensor): shape (h, w).
        """
        num_pos, h, w = pos_mask.shape
        num_neg = neg_mask.shape[0]
        num_samples = num_pos + num_neg

        labels = pos_mask.new_full((num_samples, ),
                                   self.num_classes,
                                   dtype=torch.long)
        mask_targets = pos_mask.new_zeros((num_samples, h, w))
        seg_targets = pos_mask.new_full((h, w),
                                        self.num_classes,
                                        dtype=torch.long)

        if gt_sem_cls is not None and gt_sem_seg is not None:
            gt_sem_seg = gt_sem_seg.bool()
            for sem_mask, sem_cls in zip(gt_sem_seg, gt_sem_cls):
                seg_targets[sem_mask] = sem_cls.long()

        if num_pos > 0:
            labels[pos_inds] = pos_gt_labels
            mask_targets[pos_inds] = pos_gt_mask
            for i in range(num_pos):
                seg_targets[pos_gt_mask[i].bool()] = pos_gt_labels[i]

        return labels, mask_targets, seg_targets

    def get_targets(self, sampling_results, gt_sem_seg, gt_sem_cls):
        """Compute classification and segmentation targets for all images.

        Args:
            sampling_results (list[:obj:`MaskSamplingResult`]): Mask sampling
                results.
            gt_sem_seg (list[Tensor]): Ground truth stuff class indices
                for all images. Each with shape (n, ), n is the number
                of stuff class in a image.
            gt_sem_cls (list[Tensor]): Ground truth mask of stuff
                for all images. Each with shape (n, h, w).

        Returns:
            tuple:

            - labels (Tensor): shape (batch_size * num_proposals,).
            - mask_targets (Tensor): shape (batch_size * num_proposals, h,
              w).
            - seg_targets (Tensor): shape (batch_size, h, w).
        """
        pos_inds_list = [res.pos_inds for res in sampling_results]
        pos_mask_list = [res.pos_masks for res in sampling_results]
        neg_mask_list = [res.neg_masks for res in sampling_results]
        pos_gt_mask_list = [res.pos_gt_masks for res in sampling_results]
        pos_gt_labels_list = [res.pos_gt_labels for res in sampling_results]

        results = multi_apply(self._get_targets_single, pos_inds_list,
                              pos_mask_list, neg_mask_list, pos_gt_mask_list,
                              pos_gt_labels_list, gt_sem_seg, gt_sem_cls)

        (labels, mask_targets, seg_targets) = results

        labels = torch.cat(labels, dim=0)
        mask_targets = torch.cat(mask_targets, dim=0)
        seg_targets = torch.cat(seg_targets, dim=0)

        return labels, mask_targets, seg_targets

    def loss(self, mask_preds, seg_preds, labels, mask_targets, seg_targets):
        """Loss function.

        Args:
            mask_preds (Tensor): Mask logits, shape (batch_size, num_proposals,
                h, w).
            seg_preds (Tensor): Segmentation logits, shape (batch_size,
                num_stuff_classes, h, w).
            labels (Tensor): Label targets, shape (batch_size * num_proposals).
            mask_targets (Tensor): Mask targets, shape
                (batch_size * num_proposals, h, w).
            seg_targets (Tensor): Segmentation targets, shape (batch_size, h,
                w).

        Returns:
            dict: A dictionary of loss components.
        """
        losses = dict()
        bg_class_ind = self.num_classes
        pos_inds = (labels >= 0) & (labels < bg_class_ind)
        batch_size, _, h, w = mask_preds.shape

        if pos_inds.any():
            bool_pos_inds = pos_inds.type(torch.bool)
            pos_mask_preds = mask_preds.reshape((-1, h, w))[bool_pos_inds]
            pos_mask_targets = mask_targets[bool_pos_inds]
            losses['loss_rpn_mask'] = self.loss_mask(pos_mask_preds,
                                                     pos_mask_targets)
            losses['loss_rpn_dice'] = self.loss_dice(pos_mask_preds,
                                                     pos_mask_targets)

            if self.loss_rank:
                rank_targets = mask_targets.new_full((batch_size, h, w),
                                                     self.ignore_label,
                                                     dtype=torch.long)
                rank_inds = pos_inds.view(
                    (batch_size, -1)).nonzero(as_tuple=False)
                batch_mask_targets = mask_targets.view(
                    (batch_size, -1, h, w)).bool()
                for i in range(batch_size):
                    curr_inds = (rank_inds[:, 0] == i)
                    curr_rank = rank_inds[:, 1][curr_inds]
                    for j in curr_rank:
                        rank_targets[i][batch_mask_targets[i][j]] = j
                losses['loss_rpn_rank'] = self.loss_rank(
                    mask_preds, rank_targets, ignore_index=self.ignore_label)
        else:
            losses['loss_rpn_mask'] = mask_preds.sum() * 0
            losses['loss_rpn_dice'] = mask_preds.sum() * 0
            if self.loss_rank:
                losses['loss_rpn_rank'] = mask_preds.sum() * 0

        cls_channels = seg_preds.shape[1]
        # shape (batch_size, cls_channels, h, w) ->
        # (batch_size * h * w, cls_channels)
        flatten_segs = seg_preds.view(
            (-1, cls_channels, h * w)).permute(0, 2, 1).reshape(
                (-1, cls_channels))
        flatten_seg_targets = seg_targets.view((-1, ))

        if self.loss_seg.use_sigmoid:
            num_dense_pos = (flatten_seg_targets >= 0) & (
                flatten_seg_targets < bg_class_ind)
            num_dense_pos = num_dense_pos.sum().float().clamp(min=1.0)
            losses['loss_rpn_seg'] = self.loss_seg(
                flatten_segs, flatten_seg_targets, avg_factor=num_dense_pos)
        else:
            losses['loss_rpn_seg'] = self.loss_seg(flatten_segs,
                                                   flatten_seg_targets)
        return losses

    def forward_train(self, x, img_metas, gt_masks, gt_labels, gt_sem_seg,
                      gt_sem_cls):
        """
        Args:
            x (list[Tensor]): Features from FPN.
            img_metas (list[dict]): Meta information of each image, e.g.,
                image size, scaling factor, etc.
            gt_masks (list[Tensor]): Each element of list is ground truth masks
                of instances, shape (num_instances, h, w).
            gt_labels (list[Tensor]): Each element of list is ground truth
                labels of instances, shape (num_instances,).
            gt_sem_seg (list[Tensor]): Each element of list is ground truth
                masks of stuff, shape (num_stuff, h, w). It is list[None]
                for instance segmentation.
            gt_sem_cls (list[Tensor]): Each element of list is ground truth
                labels of stuff, shape (num_stuff,). Is is list[None] for
                instance segmentation.

        Returns:
            tuple:

            - losses (dict[str, Tensor]): A dictionary of loss components.
            - x_feats (Tensor): Feature maps, shape (batch_size, c, h, w).
            - proposal_feats (Tensor): Proposal features, shape
              (batch_size, n, c, kernel_size, kernel_size).
              n is (num_proposals + num_stuff) for panoptic segmentation,
              num_proposals for instance segmentation.
            - mask_preds (Tensor): Mask logits with shape (batch_size, n,
              h, w), n is num_proposals + num_stuff for panoptic
              segmentation, num_proposals for instance segmentation.
        """
        num_imgs = len(img_metas)
        x_feats, proposal_feats, mask_preds, seg_preds = self(x, img_metas)

        if self.feat_scale_factor != 1:
            scaled_mask_preds = F.interpolate(
                mask_preds,
                scale_factor=self.feat_scale_factor,
                mode='bilinear',
                align_corners=False)
            scaled_seg_preds = F.interpolate(
                seg_preds,
                scale_factor=self.feat_scale_factor,
                mode='bilinear',
                align_corners=False)
        else:
            scaled_mask_preds = mask_preds
            scaled_seg_preds = seg_preds

        sampling_results = []
        cls_scores = [None] * num_imgs
        for i in range(num_imgs):
            assign_result = self.assigner.assign(cls_scores[i],
                                                 scaled_mask_preds[i].detach(),
                                                 gt_labels[i], gt_masks[i],
                                                 img_metas[i])
            sampling_result = self.sampler.sample(assign_result,
                                                  scaled_mask_preds[i],
                                                  gt_masks[i])
            sampling_results.append(sampling_result)

        targets = self.get_targets(
            sampling_results, gt_sem_seg=gt_sem_seg, gt_sem_cls=gt_sem_cls)

        losses = self.loss(scaled_mask_preds, scaled_seg_preds, *targets)

        if self.cat_stuff_mask:
            mask_preds, proposal_feats = self._cat_stuff_mask(
                mask_preds=mask_preds,
                seg_preds=seg_preds,
                proposal_feats=proposal_feats)

        return losses, x_feats, proposal_feats, mask_preds

    def simple_test_rpn(self, x, img_metas):
        """
        Args:
            x (tuple[Tensor]): Features from the upstream network, each is
                a 4D-tensor.
            img_metas (list[dict]): Meta info of each image.

        Returns:
            tuple[Tensor]:

            - x_feats (Tensor): Feature maps, shape (batch_size, c, h, w).
            - proposal_feats (Tensor): Proposal features, shape
              (batch_size, n, c, kernel_size, kernel_size).
              n is (num_proposals + num_stuff) for panoptic segmentation,
              num_proposals for instance segmentation.
            - mask_preds (Tensor): Mask logits with shape (batch_size, n,
              h, w), n is num_proposals + num_stuff for panoptic
              segmentation, num_proposals for instance segmentation.
        """
        x_feats, proposal_feats, mask_preds, seg_preds = self(x, img_metas)

        if self.cat_stuff_mask:
            mask_preds, proposal_feats = self._cat_stuff_mask(
                mask_preds=mask_preds,
                seg_preds=seg_preds,
                proposal_feats=proposal_feats)

        return x_feats, proposal_feats, mask_preds
