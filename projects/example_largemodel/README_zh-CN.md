# 视觉大模型实践案例

本工程用于探索如何在消费级显卡上成功训练相对大的视觉模型。

虽然视觉模型并没有像 LLM 那样有极其夸张的参数量，但是即使常用的以 Swin Large 为 backbone 的模型，都需要在 A100 上才能成功训练，这无疑阻碍了用户在视觉大模型上的探索和实验。因此本工程将探索在 3090 等 24G 甚至更小显存的消级显卡上如何训练视觉大模型。

本工程主要涉及到的训练技术有 `FSDP`、`DeepSpeed` 和 `ColossalAI` 等常用大模型训练技术。

本工程将不断更新完善，如果你有比较好的探索和意见，也非常欢迎提 PR

## 案例 1： 采用 8张 24G 3090 显卡结合 FSDP 训练 `dino-5scale_swin-l_fsdp_8xb2-12e_coco.py`

| ID | AMP | GC of Backbone | GC of Encoder | FSDP | Peak Mem (GB) | Iter Time (s) |
|:--:|:---:|:--------------:|--------------:|:----:|:-------------:|:-------------:|
| 1  |     |                |               |      |   49 (A100)   |      0.9      |
| 2  |  √  |                |               |      |   39 (A100)   |      1.2      |
| 3  |     |       √        |               |      |   33 (A100)   |      1.1      |
| 4  |  √  |       √        |               |      |   25 (A100)   |      1.3      |
| 5  |     |       √        |            √  |      |      18       |      2.2      |
| 6  |  √  |       √        |             √ |      |      13       |      1.6      |
| 7  |     |        √        |             √ |   √   |      14       |      2.9      |
| 8  |  √   |        √        |             √ |   √   |      8.5      |      2.4      |

- AMP: Automatic Mixed Precision 混合精度训练
- GC: Gradient/Activation checkpointing 梯度、激活值检查点
- FSDP: ZeRO-3 with Activation Checkpointing ZeRO-3 结合梯度检查点
- Iter Time: Total training time for one iteration. 一次迭代训练总时间

从上表可以看出：

1. 采用 FSDP 结合 AMP 和 GC 技术，可以将最初的 49G 显存降低为 8.5G，但是会增加 1.7 倍训练时间
2. 在目标检测视觉模型中，占据最大显存的是激活值，而不是优化器状态，这和 LLM 不同，因此用户应该首选梯度检查点，而不是 FSDP
3. 如果不开启梯度检查点，仅开启 FSDP 的话依然会 OOM，即使尝试了更加细致的参数切分策略
4. 虽然 AMP 可以减少不少显存，但是有些算法使用 AMP 会导致精度下降而 FSDP 不会

