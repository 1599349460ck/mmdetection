# Copyright (c) OpenMMLab. All rights reserved.
from typing import Dict, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from mmcv.cnn import ConvModule
from mmengine.model import BaseModule, bias_init_with_prob, normal_init
from mmengine.structures import InstanceData
from torch import Tensor

from mmdet.registry import MODELS, TASK_UTILS
from mmdet.structures import SampleList
from ..utils import multi_apply


@MODELS.register_module()
class KernelRPNHead(BaseModule):
    """KernelRPNHead.

    See `K-Net: Towards Unified Image Segmentation
    <https://arxiv.org/pdf/2106.14855>`_ for more details.

    Args:
        in_channels (int): Number of channels in the input feature map.
        out_channels (int): Number of channels in the output feature map.
        num_proposals (int): Number of proposals generated by RPN.
        num_things_classes (int): Number of things. Defaults to 80.
        num_stuff_classes (int): Number of stuff. Defaults to 53.
        ignore_label (int): Class indice to be ignored. Defaults to 255.
        num_cls_fcs (int): Number of fully connected layers
            in the classification branch. Defaults to 1.
        num_seg_convs (int): Number of convolution layers in the
            segmentation branch. Defaults to 1.
        num_loc_convs (int): Number of convolution layers in the
            location branch. Defaults to 1.
        localization_fpn_cfg (:obj:`mmcv.Config`): Config of semantic FPN.
        conv_kernel_size (int): Kernel size. Defaults to 1.
        norm_cfg (:obj:`mmcv.Config`): Norm config.
            Defaults to dict(type='GN', num_groups=32).
        feat_scale_factor (int): feature map scale factor.
            Defaults to 2.
        loss_rank (:obj:`mmcv.Config`): Rank loss config.
            Defaults to dict( type='CrossEntropyLoss', ...).
        loss_mask (:obj:`mmcv.Config`): Mask loss config.
            Defaults to dict( type='CrossEntropyLoss', ...).
        loss_dice (:obj:`mmcv.Config`): Dice loss config.
            Defaults to dict(type='DiceLoss', ...).
        loss_seg (:obj:`mmcv.Config`): Segmentation loss config.
            Defaults to dict( type='FocalLoss', ...).
        train_cfg (:obj:`mmcv.Config`, optional): Training config.
            Defaults to None.
        test_cfg (:obj:`mmcv.Config`, optional): Testing config.
            Defaults to None.
        init_cfg (dict or list[dict], optional): Initialization config dict.
            Defaults to None.
    """

    def __init__(
            self,
            in_channels,
            out_channels,
            num_proposals,
            num_things_classes=80,
            num_stuff_classes=53,
            # solve the problem caused by two_stage.py line 44
            num_classes=None,
            ignore_label=255,
            num_cls_fcs=1,
            num_seg_convs=1,
            num_loc_convs=1,
            localization_fpn_cfg=dict(
                type='SemanticFPN',
                in_channels=256,
                feat_channels=256,
                out_channels=256,
                start_level=0,
                end_level=3,
                output_level=1,
                positional_encoding_level=3,
                positional_encoding_cfg=dict(
                    type='SinePositionalEncoding',
                    num_feats=128,
                    normalize=True),
                add_aux_conv=True,
                out_act_cfg=dict(type='ReLU'),
                conv_cfg=None,
                norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)),
            conv_kernel_size=1,
            norm_cfg=dict(type='GN', num_groups=32),
            feat_scale_factor=2,
            loss_rank=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.1),
            loss_mask=dict(
                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
            loss_dice=dict(type='DiceLoss', loss_weight=4.0),
            loss_seg=dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                loss_weight=1.0),
            train_cfg=None,
            test_cfg=None,
            init_cfg=None):
        super().__init__(init_cfg=init_cfg)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_proposals = num_proposals

        self.num_things_classes = num_things_classes
        self.num_stuff_classes = num_stuff_classes
        self.num_classes = num_things_classes + num_stuff_classes
        self.ignore_label = ignore_label
        self.cat_stuff_mask = num_stuff_classes > 0

        self.num_cls_fcs = num_cls_fcs
        self.num_seg_convs = num_seg_convs
        self.num_loc_convs = num_loc_convs

        self.localization_fpn_cfg = localization_fpn_cfg
        self.conv_kernel_size = conv_kernel_size
        self.norm_cfg = norm_cfg

        self.feat_scale_factor = feat_scale_factor

        self.loss_rank = MODELS.build(loss_rank) if loss_rank else None
        self.loss_mask = MODELS.build(loss_mask)
        self.loss_dice = MODELS.build(loss_dice)
        self.loss_seg = MODELS.build(loss_seg)

        self.test_cfg = test_cfg
        self.train_cfg = train_cfg
        if self.train_cfg:
            self.assigner = TASK_UTILS.build(self.train_cfg['assigner'])
            self.sampler = TASK_UTILS.build(
                self.train_cfg['sampler'], default_args=dict(context=self))

        self._init_layers()

    def _init_layers(self):
        """Initialize layers."""
        self.localization_fpn = MODELS.build(self.localization_fpn_cfg)

        self.init_kernels = nn.Conv2d(
            self.out_channels,
            self.num_proposals,
            kernel_size=self.conv_kernel_size,
            padding=int(self.conv_kernel_size // 2),
            stride=1,
            bias=False)

        self.conv_seg = nn.Conv2d(
            self.out_channels,
            self.num_classes if self.loss_seg.use_sigmoid else
            (self.num_classes + 1),
            kernel_size=1)

        self.loc_convs = nn.Sequential()
        for _ in range(self.num_loc_convs):
            self.loc_convs.add_module(
                str(len(self.loc_convs)),
                ConvModule(
                    self.in_channels,
                    self.out_channels,
                    kernel_size=1,
                    padding=0,
                    stride=1,
                    norm_cfg=self.norm_cfg))

        self.seg_convs = nn.Sequential()
        for _ in range(self.num_seg_convs):
            self.seg_convs.add_module(
                str(len(self.seg_convs)),
                ConvModule(
                    self.in_channels,
                    self.out_channels,
                    kernel_size=1,
                    padding=0,
                    stride=1,
                    norm_cfg=self.norm_cfg))

    def init_weights(self):
        """Initialize weights."""
        self.localization_fpn.init_weights()

        for conv in [self.loc_convs, self.seg_convs]:
            for m in conv.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01)

        bias_seg = bias_init_with_prob(0.01)
        if self.loss_seg.use_sigmoid:
            normal_init(self.conv_seg, std=0.01, bias=bias_seg)
        else:
            normal_init(self.conv_seg, mean=0, std=0.01)

        normal_init(self.init_kernels, mean=0, std=1)

    def _cat_stuff_mask(self, mask_preds, seg_preds, proposal_feats):
        """Concatenate stuff masks and things masks togather, concatenate stuff
        kernel and things kernel togather."""
        mask_preds = torch.cat(
            [mask_preds, seg_preds[:, self.num_things_classes:]], dim=1)
        stuff_kernels = self.conv_seg.weight[self.num_things_classes:].clone()
        # shape (c_out, c_in, k, k) -> (batch_size, c_out, c_in, k, k)
        stuff_kernels = stuff_kernels[None].expand((mask_preds.shape[0], ) +
                                                   stuff_kernels.shape)
        proposal_feats = torch.cat([proposal_feats, stuff_kernels], dim=1)
        return mask_preds, proposal_feats

    def forward(self, x: Tuple[Tensor],
                batch_data_samples: SampleList) -> Tuple[Tensor]:
        """
        Args:
            x (tuple[Tensor]): Features from the upstream network, each
                is a 4D-tensor.
            batch_data_samples (List[:obj:`DetDataSample`]): The Data
                Samples. It usually includes information such as
                `gt_instance`, `gt_panoptic_seg` and `gt_sem_seg`.

        Returns:
            tuple[Tensor]: a tuple contains the following tensors:

                - x_feats (Tensor): Feature maps, shape (batch_size, c, h, w).
                - proposal_feats (Tensor): Proposal feats, shape (batch_size,
                    num_proposals, c, kernel_size, kernel_size).
                - mask_preds (Tensor): Mask logits, shape (batch_size,
                    num_proposals, h, w).
                - seg_preds (Tensor): Segmentation logits, shape
                    (batch_size, num_stuff_classes, h, w).
        """
        batch_img_metas = [
            data_sample.metainfo for data_sample in batch_data_samples
        ]
        num_imgs = len(batch_img_metas)

        loc_feats, sem_feats = self.localization_fpn(x)

        loc_feats = self.loc_convs(loc_feats)
        sem_feats = self.seg_convs(sem_feats)

        mask_preds = self.init_kernels(loc_feats)
        seg_preds = self.conv_seg(sem_feats)

        proposal_feats = self.init_kernels.weight.clone()
        # shape (c_out, c_in, kernel_size, kernel_size) ->
        # (batch_size, c_out, c_in, kernel_size, kernel_size)
        proposal_feats = proposal_feats[None].expand(num_imgs,
                                                     *proposal_feats.shape)
        if sem_feats is not None:
            x_feats = sem_feats + loc_feats
        else:
            x_feats = loc_feats

        sigmoid_masks = mask_preds.sigmoid()
        nonzeros_inds = sigmoid_masks > 0.5
        sigmoid_masks = nonzeros_inds.float()
        obj_feats = torch.einsum('bnhw,bchw->bnc', sigmoid_masks, x_feats)

        proposal_feats = proposal_feats + obj_feats.view(
            num_imgs, self.num_proposals, self.out_channels, 1, 1)

        return x_feats, proposal_feats, mask_preds, seg_preds

    def _get_targets_single(self, mask_pred, data_sample):
        """
        Args:

        Returns:
            tuple:

            - labels (Tensor): shape (num_proposals,).
            - mask_targets (Tensor): shape (num_proposals, h, w).
            - seg_targets (Tensor): shape (h, w).
        """
        img_meta = data_sample.metainfo
        gt_instances = data_sample.gt_instances
        pred_instances = InstanceData(masks=mask_pred)
        assign_result = self.assigner.assign(
            pred_instances=pred_instances,
            gt_instances=gt_instances,
            img_meta=img_meta)
        sampling_result = self.sampler.sample(
            assign_result=assign_result,
            pred_instances=pred_instances,
            gt_instances=gt_instances)

        pos_inds = sampling_result.pos_inds
        pos_mask = sampling_result.pos_masks
        neg_mask = sampling_result.neg_masks
        pos_gt_mask = sampling_result.pos_gt_masks
        pos_gt_labels = sampling_result.pos_gt_labels

        num_pos, h, w = pos_mask.shape
        num_neg = neg_mask.shape[0]
        num_samples = num_pos + num_neg

        labels = pos_mask.new_full((num_samples, ),
                                   self.num_classes,
                                   dtype=torch.long)
        mask_targets = pos_mask.new_zeros((num_samples, h, w))
        seg_targets = pos_mask.new_full((h, w),
                                        self.num_classes,
                                        dtype=torch.long)

        if 'gt_sem_instances' in data_sample:
            gt_sem_cls = data_sample.gt_sem_instances.labels
            gt_sem_seg = data_sample.gt_sem_instances.masks
            gt_sem_seg = gt_sem_seg.bool()
            for sem_mask, sem_cls in zip(gt_sem_seg, gt_sem_cls):
                seg_targets[sem_mask] = sem_cls.long()

        if num_pos > 0:
            labels[pos_inds] = pos_gt_labels
            mask_targets[pos_inds] = pos_gt_mask
            for i in range(num_pos):
                seg_targets[pos_gt_mask[i].bool()] = pos_gt_labels[i]

        return labels, mask_targets, seg_targets, sampling_result

    def get_targets(self,
                    mask_preds,
                    batch_data_samples,
                    return_sampling_results: bool = False):
        """Compute classification and segmentation targets for all images.

        Args:

        Returns:
            tuple:

            - labels (Tensor): shape (batch_size * num_proposals,).
            - mask_targets (Tensor): shape (batch_size * num_proposals, h,
                w).
            - seg_targets (Tensor): shape (batch_size, h, w).
        """
        results = multi_apply(self._get_targets_single, mask_preds,
                              batch_data_samples)
        (labels, mask_targets, seg_targets) = results[:3]
        rest_results = results[3:]

        labels = torch.cat(labels, dim=0)
        mask_targets = torch.cat(mask_targets, dim=0)
        seg_targets = torch.cat(seg_targets, dim=0)
        res = (labels, mask_targets, seg_targets)

        if return_sampling_results:
            res = res + tuple(rest_results)

        return res

    def loss_by_feat(self, mask_preds, seg_preds, labels, mask_targets,
                     seg_targets):
        """Loss function.

        Args:
            mask_preds (Tensor): Mask logits, shape (batch_size, num_proposals,
                h, w).
            seg_preds (Tensor): Segmentation logits, shape (batch_size,
                num_stuff_classes, h, w).
            labels (Tensor): Label targets, shape (batch_size * num_proposals).
            mask_targets (Tensor): Mask targets, shape
                (batch_size * num_proposals, h, w).
            seg_targets (Tensor): Segmentation targets, shape (batch_size, h,
                w).

        Returns:
            dict: A dictionary of loss components.
        """
        losses = dict()
        bg_class_ind = self.num_classes
        pos_inds = (labels >= 0) & (labels < bg_class_ind)
        batch_size, _, h, w = mask_preds.shape

        if pos_inds.any():
            bool_pos_inds = pos_inds.type(torch.bool)
            pos_mask_preds = mask_preds.reshape((-1, h, w))[bool_pos_inds]
            pos_mask_targets = mask_targets[bool_pos_inds]
            losses['loss_rpn_mask'] = self.loss_mask(pos_mask_preds,
                                                     pos_mask_targets)
            losses['loss_rpn_dice'] = self.loss_dice(pos_mask_preds,
                                                     pos_mask_targets)

            if self.loss_rank:
                rank_targets = mask_targets.new_full((batch_size, h, w),
                                                     self.ignore_label,
                                                     dtype=torch.long)
                rank_inds = pos_inds.view(
                    (batch_size, -1)).nonzero(as_tuple=False)
                batch_mask_targets = mask_targets.view(
                    (batch_size, -1, h, w)).bool()
                for i in range(batch_size):
                    curr_inds = (rank_inds[:, 0] == i)
                    curr_rank = rank_inds[:, 1][curr_inds]
                    for j in curr_rank:
                        rank_targets[i][batch_mask_targets[i][j]] = j
                losses['loss_rpn_rank'] = self.loss_rank(
                    mask_preds, rank_targets, ignore_index=self.ignore_label)
        else:
            losses['loss_rpn_mask'] = mask_preds.sum() * 0
            losses['loss_rpn_dice'] = mask_preds.sum() * 0
            if self.loss_rank:
                losses['loss_rpn_rank'] = mask_preds.sum() * 0

        cls_channels = seg_preds.shape[1]
        # shape (batch_size, cls_channels, h, w) ->
        # (batch_size * h * w, cls_channels)
        flatten_segs = seg_preds.view(
            (-1, cls_channels, h * w)).permute(0, 2, 1).reshape(
                (-1, cls_channels))
        flatten_seg_targets = seg_targets.view((-1, ))

        if self.loss_seg.use_sigmoid:
            num_dense_pos = (flatten_seg_targets >= 0) & (
                flatten_seg_targets < bg_class_ind)
            num_dense_pos = num_dense_pos.sum().float().clamp(min=1.0)
            losses['loss_rpn_seg'] = self.loss_seg(
                flatten_segs, flatten_seg_targets, avg_factor=num_dense_pos)
        else:
            losses['loss_rpn_seg'] = self.loss_seg(flatten_segs,
                                                   flatten_seg_targets)
        return losses

    def loss(self, x: Tuple[Tensor],
             batch_data_samples: SampleList) -> Dict[str, Tensor]:
        """Perform forward propagation and loss calculation of the panoptic
        head on the features of the upstream network.

        Args:
            x (tuple[Tensor]): Multi-level features from the upstream
                network, each is a 4D-tensor.
            batch_data_samples (List[:obj:`DetDataSample`]): The Data
                Samples. It usually includes information such as
                `gt_instance`, `gt_panoptic_seg` and `gt_sem_seg`.

        Returns:
            dict[str, Tensor]: a dictionary of loss components
        """
        x_feats, proposal_feats, mask_preds, seg_preds = self(
            x, batch_data_samples)

        if self.feat_scale_factor != 1:
            scaled_mask_preds = F.interpolate(
                mask_preds,
                scale_factor=self.feat_scale_factor,
                mode='bilinear',
                align_corners=False)
            scaled_seg_preds = F.interpolate(
                seg_preds,
                scale_factor=self.feat_scale_factor,
                mode='bilinear',
                align_corners=False)
        else:
            scaled_mask_preds = mask_preds
            scaled_seg_preds = seg_preds

        targets = self.get_targets(scaled_mask_preds, batch_data_samples)

        losses = self.loss_by_feat(scaled_mask_preds, scaled_seg_preds,
                                   *targets)

        if self.cat_stuff_mask:
            mask_preds, proposal_feats = self._cat_stuff_mask(
                mask_preds=mask_preds,
                seg_preds=seg_preds,
                proposal_feats=proposal_feats)

        return losses, x_feats, proposal_feats, mask_preds

    def predict(self, x: Tuple[Tensor],
                batch_data_samples: SampleList) -> Tuple[Tensor]:
        """Test without augmentaton.

        Args:
            x (tuple[Tensor]): Multi-level features from the
                upstream network, each is a 4D-tensor.
            batch_data_samples (List[:obj:`DetDataSample`]): The Data
                Samples. It usually includes information such as
                `gt_instance`, `gt_panoptic_seg` and `gt_sem_seg`.

        Returns:
            tuple[Tensor]:

            - x_feats (Tensor): Feature maps, shape (batch_size, c, h, w).
            - proposal_feats (Tensor): Proposal features, shape
                (batch_size, n, c, kernel_size, kernel_size).
                n is (num_proposals + num_stuff) for panoptic segmentation,
                num_proposals for instance segmentation.
            - mask_preds (Tensor): Mask logits with shape (batch_size, n,
                h, w), n is num_proposals + num_stuff for panoptic
                segmentation, num_proposals for instance segmentation.
        """
        x_feats, proposal_feats, mask_preds, seg_preds = self(
            x, batch_data_samples)

        if self.cat_stuff_mask:
            mask_preds, proposal_feats = self._cat_stuff_mask(
                mask_preds=mask_preds,
                seg_preds=seg_preds,
                proposal_feats=proposal_feats)

        return x_feats, proposal_feats, mask_preds
