{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet level_outputs:\n",
      "(1, 256, 32, 32)\n",
      "(1, 512, 16, 16)\n",
      "(1, 1024, 8, 8)\n",
      "(1, 2048, 4, 4)\n",
      "resnet level_outputs:\n",
      "(1, 128, 16, 16)\n",
      "(1, 128, 8, 8)\n",
      "(1, 128, 4, 4)\n",
      "(1, 128, 2, 2)\n",
      "(1, 128, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from mmdet.models import ResNet, FPN\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "setup_seed(0)\n",
    "\n",
    "# out_indices=(0, 1, 2, 3)\n",
    "backbone = ResNet( depth=50,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "#         norm_cfg = dict(type='BN', requires_grad=True),\n",
    "#         dcn=dict(type='DCN', deform_groups=1, fallback_on_stride=False),\n",
    "#         stage_with_dcn=(False, False, True, True)\n",
    "        )\n",
    "backbone.eval()\n",
    "inputs = torch.rand(1, 3, 128, 128)\n",
    "# net.to(\"cuda\")\n",
    "# inputs = inputs.to(\"cuda\")\n",
    "backbone_outputs = backbone(inputs)\n",
    "print(\"resnet level_outputs:\")\n",
    "for backbone_output in backbone_outputs:\n",
    "    print(tuple(backbone_output.shape))\n",
    "neck = FPN(in_channels=[256, 512, 1024, 2048],\n",
    "             out_channels=128,\n",
    "             start_level=1,\n",
    "             end_level=-1,\n",
    "             num_outs=5,\n",
    "             init_cfg=dict(type='Xavier', layer='Conv2d', distribution='uniform'))\n",
    "\n",
    "neck.eval()\n",
    "neck_outputs = neck(backbone_outputs)\n",
    "print(\"resnet level_outputs:\")\n",
    "for neck_output in neck_outputs:\n",
    "    print(tuple(neck_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet level_outputs:\n",
      "(1, 256, 200, 288)\n",
      "(1, 512, 100, 144)\n",
      "(1, 1024, 50, 72)\n",
      "(1, 2048, 25, 36)\n",
      "neck level_outputs:\n",
      "(1, 128, 100, 144)\n",
      "(1, 128, 50, 72)\n",
      "(1, 128, 25, 36)\n",
      "(1, 128, 13, 18)\n",
      "(1, 128, 7, 9)\n"
     ]
    }
   ],
   "source": [
    "from mmdet.models import Res2Net, BiFPN\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "setup_seed(0)\n",
    "\n",
    "# out_indices=(0, 1, 2, 3)\n",
    "backbone = Res2Net( depth=101,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "#         norm_cfg = dict(type='BN', requires_grad=True),\n",
    "#         dcn=dict(type='DCN', deform_groups=1, fallback_on_stride=False),\n",
    "        stage_with_dcn=(False, False, True, True))\n",
    "backbone.eval()\n",
    "inputs = torch.rand(1, 3, 800, 1152)\n",
    "# net.to(\"cuda\")\n",
    "# inputs = inputs.to(\"cuda\")\n",
    "backbone_outputs = backbone(inputs)\n",
    "print(\"resnet level_outputs:\")\n",
    "for backbone_output in backbone_outputs:\n",
    "    print(tuple(backbone_output.shape))\n",
    "neck = BiFPN(in_channels=[256, 512, 1024, 2048],\n",
    "             out_channels=128,\n",
    "             start_level=1,\n",
    "             end_level=3,\n",
    "             num_outs=5,\n",
    "             num_bifpn=1,\n",
    "             init_cfg=dict(type='Xavier', layer='Conv2d', distribution='uniform'))\n",
    "neck.init_weights()\n",
    "neck.eval()\n",
    "neck_outputs = neck(backbone_outputs)\n",
    "print(\"neck level_outputs:\")\n",
    "for neck_output in neck_outputs:\n",
    "    print(tuple(neck_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.cnn import ConvModule, Scale\n",
    "from mmdet.core import multi_apply\n",
    "import torch.nn as nn\n",
    "from mmdet.models.builder import HEADS, build_loss\n",
    "from mmdet.models.dense_heads.base_dense_head import BaseDenseHead\n",
    "from mmdet.models.dense_heads.dense_test_mixins import BBoxTestMixin\n",
    "\n",
    "INF = 1e8\n",
    "\n",
    "class CenterNetV2Head(BaseDenseHead, BBoxTestMixin):\n",
    "    \"\"\"Objects as Points Head. CenterV2Head use center_point to indicate object's\n",
    "    position. Paper link <https://arxiv.org/abs/2103.07461>\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channel in the input feature map.\n",
    "        feat_channels (int): Number of channel in the intermediate feature map.\n",
    "        stacked_convs (int):\n",
    "        strides (list or tuple[int])\n",
    "        regress_ranges (tuple[tuple[int, int]]):\n",
    "        dcn_on_last_conv (bool):\n",
    "        loss_heatmap:\n",
    "        loss_bbox:\n",
    "        conv_cfg (dict):\n",
    "        norm_cfg (dict):\n",
    "        init_cfg (dict or list[dict], optional): Initialization config dict.\n",
    "            Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 feat_channels,\n",
    "                 stacked_convs=4,\n",
    "                 strides=(8, 16, 32, 64, 128),\n",
    "                 regress_ranges=((-1, 64), (64, 128), (128, 256),\n",
    "                                 (256, 512), (512, INF),),\n",
    "                 dcn_on_last_conv=True,\n",
    "                 loss_heatmap=dict(\n",
    "                     type='FocalLoss',\n",
    "                     use_sigmoid=True,\n",
    "                     gamma=2.0,\n",
    "                     alpha=0.25,\n",
    "                     loss_weight=1.0),\n",
    "                 loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=None,\n",
    "                 init_cfg=None):\n",
    "        super(CenterNetV2Head, self).__init__()\n",
    "        self.regress_ranges = regress_ranges\n",
    "        self.in_channels = in_channels\n",
    "        self.feat_channels = feat_channels\n",
    "        self.stacked_convs = stacked_convs\n",
    "        self.strides = strides\n",
    "        self.regress_ranges = regress_ranges\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.dcn_on_last_conv = dcn_on_last_conv\n",
    "        self._init_layers()\n",
    "        self.loss_heatmap = build_loss(loss_heatmap)\n",
    "        self.loss_bbox = build_loss(loss_bbox)\n",
    "\n",
    "    def _init_layers(self):\n",
    "        \"\"\"Initialize layers of the head.\"\"\"\n",
    "        self._init_convs()\n",
    "        self._init_predictor()\n",
    "        self.scales = nn.ModuleList([Scale(1.0) for _ in self.regress_ranges])\n",
    "\n",
    "    def _init_convs(self):\n",
    "        \"\"\"Build head for each branch.\"\"\"\n",
    "        \n",
    "        for name in ['cls', 'reg']:\n",
    "            tower = nn.ModuleList()\n",
    "            for i in range(self.stacked_convs):\n",
    "                in_channel = self.in_channels if i == 0 else self.feat_channels\n",
    "                if self.dcn_on_last_conv and i == self.stacked_convs - 1:\n",
    "                    conv_cfg = dict(type='DCNv2')\n",
    "                else:\n",
    "                    conv_cfg = self.conv_cfg\n",
    "                tower.append(\n",
    "                    ConvModule(\n",
    "                        in_channel,\n",
    "                        self.feat_channels,\n",
    "                        3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg))\n",
    "            self.add_module(f'{name}_convs', tower)\n",
    "\n",
    "    def _init_predictor(self):\n",
    "        # background or foreground, class agnostic heatmap\n",
    "        self.heatmap_head = nn.Conv2d(self.feat_channels, 1, 3, padding=1)\n",
    "        self.bbox_head = nn.Conv2d(self.feat_channels, 4, 3, padding=1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        \"\"\"Forward features.\n",
    "        Args:\n",
    "            feats (tuple[Tensor]): Features from the upstream network, each is\n",
    "                a 4D-tensor.\n",
    "        Returns:\n",
    "            center_heatmap_preds (List[Tensor]): center predict heatmaps for\n",
    "                all levels, class agnostic, which means out channel is 1.\n",
    "            bbox_preds (List[Tensor]): Box energies / deltas for all scale\n",
    "            levels, each is a 4D-tensor, the channel number is num_points * 4.\n",
    "        \"\"\"\n",
    "        return multi_apply(self.forward_single, feats)\n",
    "\n",
    "    def forward_single(self, feat):\n",
    "        \"\"\"Forward a single level feature.\n",
    "        Args:\n",
    "            feat (tuple[Tensor]): Features from the upstream network, each is\n",
    "                a 4D-tensor.\n",
    "        Returns:\n",
    "            center_heatmap_preds (List[Tensor]): center predict heatmaps for\n",
    "                one level, class agnostic, which means out channel is 1.\n",
    "            bbox_preds (List[Tensor]): Box energies / deltas for one scale\n",
    "            level, each is a 4D-tensor, the channel number is num_points * 4.\n",
    "        \"\"\"\n",
    "        center_heatmap_preds = self.heatmap_head(feat).sigmoid()\n",
    "        bbox_preds = self.bbox_head(feat)\n",
    "        return center_heatmap_preds, bbox_preds\n",
    "    \n",
    "    def get_bboxes():\n",
    "        NotImplementedError\n",
    "        \n",
    "    def get_targets():\n",
    "        NotImplementedError    \n",
    "    \n",
    "    def loss():\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100, 144]) torch.Size([1, 4, 100, 144])\n",
      "torch.Size([1, 1, 50, 72]) torch.Size([1, 4, 50, 72])\n",
      "torch.Size([1, 1, 25, 36]) torch.Size([1, 4, 25, 36])\n",
      "torch.Size([1, 1, 13, 18]) torch.Size([1, 4, 13, 18])\n",
      "torch.Size([1, 1, 7, 9]) torch.Size([1, 4, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "rpn = CenterNetV2Head(128,128)\n",
    "center_heatmap_preds, bbox_preds = rpn(neck_outputs)\n",
    "for center_heatmap_pred, bbox_pred in zip(center_heatmap_preds, bbox_preds):\n",
    "    print(center_heatmap_pred.shape, bbox_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas = [{'filename': '/amiintellect/turing/young/competition/mmdetection/works/data/oil/images/qx-307.png', 'ori_filename': 'qx-307.png', 'ori_shape': (495, 701, 3), 'img_shape': (800, 1133, 3), 'pad_shape': (800, 1152, 3), 'scale_factor': np.array([1.6162624, 1.6161616, 1.6162624, 1.6161616], dtype=np.float32), 'flip': False, 'flip_direction': None, 'img_norm_cfg': {'mean': np.array([102.9801, 115.9465, 122.7717], dtype=np.float32), 'std': np.array([1., 1., 1.], dtype=np.float32), 'to_rgb': False}}]\n",
    "neck_outputs = neck_outputs\n",
    "featmap_sizes = []\n",
    "device = 'cpu'\n",
    "gt_bboxes = [torch.Tensor([[ 408.0027,  315.4001,  611.0715,  372.3802],\n",
    "        [ 645.2617,  332.2336,  850.4027,  387.9202],\n",
    "        [ 885.6290,  345.4442, 1093.8781,  402.4242],\n",
    "        [ 346.2424,  328.8053,  366.4457,  422.1526],\n",
    "        [ 398.5853,  388.6484,  601.4492,  445.4065],\n",
    "        [ 637.1537,  403.9441,  840.6491,  461.7406],\n",
    "        [ 876.7070,  416.5464, 1083.5500,  477.1525],\n",
    "        [ 390.0223,  460.6703,  598.7449,  520.6074],\n",
    "        [ 627.0999,  475.6545,  832.0861,  535.1974],\n",
    "        [ 871.8967,  489.6585, 1077.7776,  547.3785],\n",
    "        [ 863.1270,  564.4899, 1066.9784,  629.7786],\n",
    "        [ 209.1294,  639.4113,  306.5333,  675.3246],\n",
    "        [ 833.9768,  591.5006,  854.5640,  608.3724],\n",
    "        [ 797.9386,  602.4206,  813.9556,  626.0806]])]\n",
    "\n",
    "agn_hms = center_heatmap_preds\n",
    "bbox_preds = bbox_preds\n",
    "strides = (8, 16, 32, 64, 128) \n",
    "regress_ranges=((-1, 64), (64, 128), (128, 256), (256, 512), (512, INF),)\n",
    "center_sample_radius = 1.5\n",
    "norm_on_bbox = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_points(featmap_sizes, dtype, device, flatten=False):\n",
    "        \"\"\"Get points according to feature map sizes.\n",
    "\n",
    "        Args:\n",
    "            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n",
    "            dtype (torch.dtype): Type of points.\n",
    "            device (torch.device): Device of points.\n",
    "\n",
    "        Returns:\n",
    "            tuple: points of each image.\n",
    "        \"\"\"\n",
    "        mlvl_points = []\n",
    "        for i in range(len(featmap_sizes)):\n",
    "            mlvl_points.append(\n",
    "                _get_points_single(featmap_sizes[i], strides[i], dtype, device, flatten))\n",
    "        return mlvl_points\n",
    "\n",
    "\n",
    "def _get_points_single(featmap_size, stride, dtype, device, flatten=False):\n",
    "    \"\"\"Get points of a single scale level.\"\"\"\n",
    "    h, w = featmap_size\n",
    "    # First create Range with the default dtype, than convert to\n",
    "    # target `dtype` for onnx exporting.\n",
    "    x_range = torch.arange(w, device=device).to(dtype)\n",
    "    y_range = torch.arange(h, device=device).to(dtype)\n",
    "    y, x = torch.meshgrid(y_range, x_range)\n",
    "    if flatten:\n",
    "        y = y.flatten()\n",
    "        x = x.flatten()\n",
    "    points = torch.stack((x.reshape(-1) * stride, y.reshape(-1) * stride), dim=-1) + stride // 2\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.core import distance2bbox, multi_apply, multiclass_nms, reduce_mean\n",
    "\n",
    "def _get_label_indices(gt_bboxes_list, feature_map_sizes):\n",
    "    \n",
    "    self_strides = (8, 16, 32, 64, 128) \n",
    "    \n",
    "    pos_indices = []\n",
    "    L = len(self_strides)   ############ self\n",
    "    B = len(gt_bboxes_list)\n",
    "    shapes_per_level = torch.tensor(feature_map_sizes).long()\n",
    "    loc_per_level = (shapes_per_level[:, 0] * shapes_per_level[:, 1]).long() # L\n",
    "    level_bases = []\n",
    "    s = 0\n",
    "    for l in range(L):\n",
    "        level_bases.append(s)\n",
    "        s = s + B * loc_per_level[l]\n",
    "    level_bases = shapes_per_level.new_tensor(level_bases) # L    \n",
    "    strides_default = level_bases.new_tensor(self_strides).float() # L ############ self\n",
    "    for im_i in range(B):\n",
    "        bboxes = gt_bboxes_list[im_i]\n",
    "        n = bboxes.shape[0]\n",
    "        centers = ((bboxes[:, [0, 1]] + bboxes[:, [2, 3]]) / 2)\n",
    "        centers = centers.view(n, 1, 2).expand(n, L, 2)\n",
    "        strides = strides_default.view(1, L, 1).expand(n, L, 2)\n",
    "        centers_inds = (centers / strides).long()\n",
    "        Ws = shapes_per_level[:, 1].view(1, L).expand(n, L)\n",
    "        pos_ind = level_bases.view(1, L).expand(n, L) + \\\n",
    "                   im_i * loc_per_level.view(1, L).expand(n, L) + \\\n",
    "                   centers_inds[:, :, 1] * Ws + \\\n",
    "                   centers_inds[:, :, 0] # n x L\n",
    "        is_cared_in_the_level = _assign_fpn_level(bboxes)  # ################# self\n",
    "        pos_ind = pos_ind[is_cared_in_the_level].view(-1)\n",
    "        pos_indices.append(pos_ind)\n",
    "    pos_indices = torch.cat(pos_indices, dim=0).long()\n",
    "    return pos_indices\n",
    "    \n",
    "def _assign_fpn_level(boxes):\n",
    "    \n",
    "    self_regress_ranges = ((-1, 64), (64, 128), (128, 256), (256, 512), (512, INF),)\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        boxes: n x 4\n",
    "        size_ranges: L x 2\n",
    "    Return:\n",
    "        is_cared_in_the_level: n x L\n",
    "    '''\n",
    "    size_ranges = boxes.new_tensor(\n",
    "        self_regress_ranges).view(len(self_regress_ranges), 2) \n",
    "    crit = ((boxes[:, 2:] - boxes[:, :2]) **2).sum(dim=1) ** 0.5 / 2 # n\n",
    "    n, L = crit.shape[0], size_ranges.shape[0]\n",
    "    crit = crit.view(n, 1).expand(n, L)\n",
    "    size_ranges_expand = size_ranges.view(1, L, 2).expand(n, L, 2)\n",
    "    is_cared_in_the_level = (crit >= size_ranges_expand[:, :, 0]) & \\\n",
    "        (crit <= size_ranges_expand[:, :, 1])\n",
    "    return is_cared_in_the_level\n",
    "\n",
    "def get_center3x3(locations, centers, strides):\n",
    "        '''\n",
    "        Inputs:\n",
    "            locations: M x 2\n",
    "            centers: N x 2\n",
    "            strides: M\n",
    "        '''\n",
    "        M, N = locations.shape[0], centers.shape[0]\n",
    "        locations_expanded = locations.view(M, 1, 2).expand(M, N, 2) # M x N x 2\n",
    "        centers_expanded = centers.view(1, N, 2).expand(M, N, 2) # M x N x 2\n",
    "        strides_expanded = strides.view(M, 1, 1).expand(M, N, 2) # M x N\n",
    "        centers_discret = ((centers_expanded / strides_expanded).int() * \\\n",
    "            strides_expanded).float() + strides_expanded / 2 # M x N x 2\n",
    "        dist_x = (locations_expanded[:, :, 0] - centers_discret[:, :, 0]).abs()\n",
    "        dist_y = (locations_expanded[:, :, 1] - centers_discret[:, :, 1]).abs()\n",
    "        return (dist_x <= strides_expanded[:, :, 0]) & \\\n",
    "            (dist_y <= strides_expanded[:, :, 0])\n",
    "\n",
    "def assign_reg_fpn(reg_targets_per_im, size_ranges):\n",
    "        '''\n",
    "        TODO (Xingyi): merge it with assign_fpn_level\n",
    "        Inputs:\n",
    "            reg_targets_per_im: M x N x 4\n",
    "            size_ranges: M x 2\n",
    "        '''\n",
    "        crit = ((reg_targets_per_im[:, :, :2] + \\\n",
    "            reg_targets_per_im[:, :, 2:])**2).sum(dim=2) ** 0.5 / 2 # M x N\n",
    "        is_cared_in_the_level = (crit >= size_ranges[:, [0]]) & \\\n",
    "            (crit <= size_ranges[:, [1]])\n",
    "        return is_cared_in_the_level\n",
    "\n",
    "def _get_reg_targets(reg_targets, dist, mask, area):\n",
    "        '''\n",
    "          reg_targets (M x N x 4): long tensor\n",
    "          dist (M x N)\n",
    "          is_*: M x N\n",
    "        '''\n",
    "        dist[mask == 0] = INF * 1.0\n",
    "        min_dist, min_inds = dist.min(dim=1) # M\n",
    "        reg_targets_per_im = reg_targets[\n",
    "            range(len(reg_targets)), min_inds] # M x N x 4 --> M x 4\n",
    "        reg_targets_per_im[min_dist == INF] = - INF\n",
    "        return reg_targets_per_im\n",
    "    \n",
    "def _create_agn_heatmaps_from_dist(dist):\n",
    "        '''\n",
    "        TODO (Xingyi): merge it with _create_heatmaps_from_dist\n",
    "        dist: M x N\n",
    "        return:\n",
    "          heatmaps: M x 1\n",
    "        '''\n",
    "        heatmaps = dist.new_zeros((dist.shape[0], 1))\n",
    "        heatmaps[:, 0] = torch.exp(-dist.min(dim=1)[0])\n",
    "        zeros = heatmaps < 1e-4\n",
    "        heatmaps[zeros] = 0\n",
    "        return heatmaps \n",
    "    \n",
    "def _transpose(training_targets, num_loc_list):\n",
    "    '''\n",
    "    This function is used to transpose image first training targets to \n",
    "        level first ones\n",
    "    :return: level first training targets\n",
    "    '''\n",
    "    for im_i in range(len(training_targets)):\n",
    "        training_targets[im_i] = torch.split(\n",
    "            training_targets[im_i], num_loc_list, dim=0)\n",
    "\n",
    "    targets_level_first = []\n",
    "    for targets_per_level in zip(*training_targets):\n",
    "        targets_level_first.append(\n",
    "            torch.cat(targets_per_level, dim=0))\n",
    "    return targets_level_first    \n",
    "    \n",
    "def get_targets(points, feature_map_sizes, gt_bboxes_list):\n",
    "    \n",
    "        self_strides = (8, 16, 32, 64, 128) \n",
    "        self_regress_ranges = ((-1, 64), (64, 128), (128, 256), (256, 512), (512, INF),)\n",
    "        self_get_center3x3 = get_center3x3\n",
    "        self_assign_reg_fpn = assign_reg_fpn\n",
    "        self_hm_min_overlap = 0.8\n",
    "        self_min_radius = 4\n",
    "        self__get_reg_targets = _get_reg_targets\n",
    "        self__create_agn_heatmaps_from_dist = _create_agn_heatmaps_from_dist\n",
    "    \n",
    "        \"\"\"Compute regression and classification for points in multiple images.\n",
    "\n",
    "        Args:\n",
    "            points (list[Tensor]): Points of each fpn level, each has shape\n",
    "                (num_points, 2).\n",
    "            feature_map_sizes (list[torch.Size[2]]):\n",
    "            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,\n",
    "                each has shape (num_gt, 4).\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get positive pixel index\n",
    "        pos_indices = _get_label_indices(gt_bboxes_list, feature_map_sizes)\n",
    "        L = len(points)\n",
    "        num_loc_list = [len(loc) for loc in points]\n",
    "        strides = torch.cat([\n",
    "            points[0].new_ones(num_loc_list[l]) * self_strides[l] for l in range(L)]).float() # M\n",
    "        reg_size_ranges = torch.cat([points[0].new_tensor(self_regress_ranges[l]).float().view(\n",
    "            1, 2).expand(num_loc_list[l], 2) for l in range(L)])\n",
    "        points = torch.cat(points, dim=0) # M x 2\n",
    "        M =  points.shape[0]\n",
    "        reg_targets = []\n",
    "        flattened_hms = []\n",
    "        for i in range(len(gt_bboxes_list)):\n",
    "            boxes = gt_bboxes_list[i]\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            N = boxes.shape[0]\n",
    "            if N == 0:\n",
    "                reg_targets.append(points.new_zeros((M, 4)) - INF)\n",
    "                flattened_hms.append(points.new_zeros((M, 1)))\n",
    "                continue\n",
    "            l = points[:, 0].view(M, 1) - boxes[:, 0].view(1, N) # M x N\n",
    "            t = points[:, 1].view(M, 1) - boxes[:, 1].view(1, N) # M x N\n",
    "            r = boxes[:, 2].view(1, N) - points[:, 0].view(M, 1) # M x N\n",
    "            b = boxes[:, 3].view(1, N) - points[:, 1].view(M, 1) # M x N\n",
    "            reg_target = torch.stack([l, t, r, b], dim=2) # M x N x 4\n",
    "            \n",
    "            centers = ((boxes[:, [0, 1]] + boxes[:, [2, 3]]) / 2) # N x 2\n",
    "            centers_expanded = centers.view(1, N, 2).expand(M, N, 2) # M x N x 2\n",
    "            strides_expanded = strides.view(M, 1, 1).expand(M, N, 2)\n",
    "            centers_discret = ((centers_expanded / strides_expanded).int() * \\\n",
    "                strides_expanded).float() + strides_expanded / 2 # M x N x 2\n",
    "            \n",
    "            is_peak = (((points.view(M, 1, 2).expand(M, N, 2) - \\\n",
    "                centers_discret) ** 2).sum(dim=2) == 0) # M x N\n",
    "            is_in_boxes = reg_target.min(dim=2)[0] > 0 # M x N\n",
    "            is_center3x3 = self_get_center3x3(points, centers, strides) & is_in_boxes # M x N\n",
    "            is_cared_in_the_level = self_assign_reg_fpn(reg_target, reg_size_ranges) \n",
    "            reg_mask = is_center3x3 & is_cared_in_the_level\n",
    "            \n",
    "            dist2 = ((points.view(M, 1, 2).expand(M, N, 2) - \\\n",
    "                centers_expanded) ** 2).sum(dim=2) # M x N\n",
    "            dist2[is_peak] = 0\n",
    "            delta = (1 - self_hm_min_overlap) / (1 + self_hm_min_overlap)\n",
    "            radius2 = delta ** 2 * 2 * area # N\n",
    "            radius2 = torch.clamp(radius2, min=self_min_radius ** 2)\n",
    "            weighted_dist2 = dist2 / radius2.view(1, N).expand(M, N)\n",
    "            \n",
    "            reg_target = self__get_reg_targets(reg_target, weighted_dist2.clone(), reg_mask, area) \n",
    "            flattened_hm = self__create_agn_heatmaps_from_dist(weighted_dist2.clone())\n",
    "            \n",
    "            reg_targets.append(reg_target)\n",
    "            flattened_hms.append(flattened_hm)\n",
    "        \n",
    "        # transpose im first training_targets to level first ones\n",
    "        reg_targets = _transpose(reg_targets, num_loc_list)\n",
    "        flattened_hms = _transpose(flattened_hms, num_loc_list)\n",
    "        \n",
    "        for l in range(len(reg_targets)):\n",
    "            reg_targets[l] = reg_targets[l] / float(self_strides[l])\n",
    "        reg_targets = torch.cat([x for x in reg_targets], dim=0) # MB x 4\n",
    "        flattened_hms = torch.cat([x for x in flattened_hms], dim=0) # MB x C\n",
    "        \n",
    "        return pos_indices,  reg_targets, flattened_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses(pos_indices, reg_targets, flattened_hms, reg_pred, agn_hm_pred):\n",
    "    \n",
    "        self_not_norm_reg = False\n",
    "        from detectron2.utils.comm import get_world_size\n",
    "        from mmdet.models.builder import build_loss\n",
    "        from mmdet.core import distance2bbox, multi_apply, multiclass_nms, reduce_mean\n",
    "        \n",
    "        loss_hm=dict(type='BinaryFocalLoss',\n",
    "              alpha = 0.25,\n",
    "              beta = 4,\n",
    "              gamma = 2,\n",
    "              pos_weight = 0.5,\n",
    "              neg_weight = 0.5,\n",
    "              sigmoid_clamp = 1e-4,\n",
    "              ignore_high_fp = 0.85)\n",
    "        loss_bbox=dict(type='GIoULoss',\n",
    "                      loss_weight=1.0)\n",
    "        self_loss_hm = build_loss(loss_hm)\n",
    "        self_loss_bbox = build_loss(loss_bbox)\n",
    "        \n",
    "        def reduce_sum(tensor):\n",
    "            world_size = get_world_size()\n",
    "            if world_size < 2:\n",
    "                return tensor\n",
    "            tensor = tensor.clone()\n",
    "            torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
    "            return tensor\n",
    "        \n",
    "        '''\n",
    "        Inputs:\n",
    "            pos_indices: N\n",
    "            reg_targets: M x 4\n",
    "            flattened_hms: M x C\n",
    "            reg_pred: M x 4\n",
    "            agn_hm_pred: M x 1 or None\n",
    "            N: number of positive locations in all images\n",
    "            M: number of pixels from all FPN levels\n",
    "            C: number of classes\n",
    "        '''\n",
    "        \n",
    "        agn_hm_pred = torch.cat([x.permute(0, 2, 3, 1).reshape(-1) for x in agn_hm_pred], dim=0) \n",
    "        reg_pred = torch.cat([x.permute(0, 2, 3, 1).reshape(-1, 4) for x in reg_pred], dim=0)\n",
    "        \n",
    "    \n",
    "        num_pos_local = pos_indices.numel()\n",
    "        num_gpus = get_world_size()\n",
    "        total_num_pos = reduce_sum(\n",
    "            pos_indices.new_tensor([num_pos_local])).item()\n",
    "        num_pos_avg = max(total_num_pos / num_gpus, 1.0)\n",
    "        \n",
    "        reg_inds = torch.nonzero(reg_targets.max(dim=1)[0] >= 0).squeeze(1)\n",
    "        reg_pred = reg_pred[reg_inds]\n",
    "        \n",
    "        reg_targets_pos = reg_targets[reg_inds]\n",
    "        reg_weight_map = flattened_hms.max(dim=1)[0]\n",
    "        reg_weight_map = reg_weight_map[reg_inds]\n",
    "        reg_weight_map = reg_weight_map * 0 + 1 \\\n",
    "            if self_not_norm_reg else reg_weight_map\n",
    "        reg_norm = max(reduce_sum(reg_weight_map.sum()).item() / num_gpus, 1)\n",
    "        reg_loss = self_loss_bbox(reg_pred, \n",
    "                                  reg_targets_pos, \n",
    "                                  reg_weight_map,\n",
    "                                  avg_factor=reg_norm)\n",
    "\n",
    "        \n",
    "        cat_agn_heatmap = flattened_hms.max(dim=1)[0] # M\n",
    "        loss_hm = self_loss_hm(agn_hm_pred, cat_agn_heatmap, pos_indices, avg_factor=num_pos_avg)\n",
    "        \n",
    "        return dict(loss_hm=loss_hm, loss_bbox=reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682 hm samples\n",
      "120 bbox samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss_hm': tensor(190.4738, grad_fn=<DivBackward0>),\n",
       " 'loss_bbox': tensor(1.9135, grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map_sizes = [featmap.size()[-2:] for featmap in agn_hms]\n",
    "all_level_points = get_points(feature_map_sizes, bbox_preds[0].dtype, bbox_preds[0].device)\n",
    "pos_indices,  reg_targets, flattened_hms = get_targets(all_level_points, feature_map_sizes, gt_bboxes)\n",
    "print(f'{int((flattened_hms.reshape(-1)>0).int().sum())} hm samples' )\n",
    "print(f'{int((reg_targets.max(-1)[0].reshape(-1)>= 0).int().sum())} bbox samples')\n",
    "losses(pos_indices,  reg_targets, flattened_hms, bbox_preds, agn_hms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_level(points, heatmap, reg_pred, img_metas, cfg=None):\n",
    "    \n",
    "    cfg = dict(\n",
    "            nms_pre=4000,\n",
    "            max_per_img=2000,\n",
    "            score_thr=0.05,\n",
    "            nms=dict(type='nms', iou_threshold=0.7),\n",
    "            min_bbox_size=0)\n",
    "    \n",
    "    N, C, H, W = heatmap.shape\n",
    "    image_sizes = [img_meta['img_shape'] for img_meta in img_metas]\n",
    "    heatmap = heatmap.reshape(N, -1, C) # N x HW x C\n",
    "    box_regression = reg_pred.view(N, 4, H, W).permute(0, 2, 3, 1) # N x H x W x 4 \n",
    "    box_regression = box_regression.reshape(N, -1, 4)\n",
    "    \n",
    "    candidate_inds = heatmap > cfg.get('score_thr', -1)\n",
    "    pre_nms_top_n = candidate_inds.view(N, -1).sum(1) # N\n",
    "    pre_nms_topk = cfg.get('nms_pre', 1000)\n",
    "    pre_nms_top_n = pre_nms_top_n.clamp(max=pre_nms_topk)\n",
    "      \n",
    "    result = []\n",
    "    for i in range(N):\n",
    "        per_box_cls = heatmap[i] # HW x C\n",
    "        per_candidate_inds = candidate_inds[i] # n\n",
    "        per_box_cls = per_box_cls[per_candidate_inds] # n\n",
    "\n",
    "        per_candidate_nonzeros = per_candidate_inds.nonzero() # n\n",
    "        per_box_loc = per_candidate_nonzeros[:, 0] # n\n",
    "        per_class = per_candidate_nonzeros[:, 1] # n\n",
    "\n",
    "        per_box_regression = box_regression[i] # HW x 4\n",
    "        per_box_regression = per_box_regression[per_box_loc] # n x 4\n",
    "        per_points = points[per_box_loc] # n x 2\n",
    "\n",
    "        pre_nms_top_n = pre_nms_top_n[i] # 1\n",
    "\n",
    "        if per_candidate_inds.sum().item() > pre_nms_top_n.item():\n",
    "            per_box_cls, top_k_indices = \\\n",
    "                per_box_cls.topk(pre_nms_top_n, sorted=False)\n",
    "            per_class = per_class[top_k_indices]\n",
    "            per_box_regression = per_box_regression[top_k_indices]\n",
    "            per_points = per_points[top_k_indices]\n",
    "\n",
    "        detections = torch.stack([\n",
    "            per_points[:, 0] - per_box_regression[:, 0],\n",
    "            per_points[:, 1] - per_box_regression[:, 1],\n",
    "            per_points[:, 0] + per_box_regression[:, 2],\n",
    "            per_points[:, 1] + per_box_regression[:, 3],\n",
    "        ], dim=1) # n x 4\n",
    "        \n",
    "        # avoid invalid boxes in RoI heads\n",
    "        detections[:, 2] = torch.max(detections[:, 2], detections[:, 0] + 0.01)\n",
    "        detections[:, 3] = torch.max(detections[:, 3], detections[:, 1] + 0.01)\n",
    "        \n",
    "        \n",
    "        result.append(dict(proposals=detections,score=per_box_cls))\n",
    "    return result   \n",
    "        \n",
    "            \n",
    "reg_pred_per_level = bbox_preds\n",
    "agn_hm_pred_per_level = agn_hms\n",
    "points = all_level_points\n",
    "sampled_boxes = []\n",
    "\n",
    "for l in range(len(points)):\n",
    "    sampled_boxes.append(predict_single_level(points[l],agn_hms[l], bbox_preds[l], img_metas))     \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4262e+03,  1.6850e+03,  1.7673e+02, -1.0880e+03,  1.5962e+00],\n",
       "         [ 1.2151e+03, -2.9039e+03, -1.6933e+03,  2.3907e+02,  9.5319e-01],\n",
       "         [ 1.2713e+02,  2.0146e+03,  2.9388e+02, -7.0277e+02,  9.2745e-01],\n",
       "         [-9.1963e+02, -2.6573e+02,  1.2963e+03,  1.5221e+03,  5.0000e-01],\n",
       "         [-1.6174e+02, -1.1654e+03,  4.3114e+01, -2.5149e+03,  5.0000e-01],\n",
       "         [ 1.1953e+03, -1.7077e+03, -1.0109e+03,  1.3001e+03,  5.0000e-01],\n",
       "         [ 2.4223e+03,  1.2864e+01,  8.8145e+02,  9.2308e+01,  5.0000e-01],\n",
       "         [-1.6230e+02,  2.1452e+03, -2.8130e+02, -1.9562e+03,  5.0000e-01],\n",
       "         [ 1.9333e+03, -1.0536e+03,  3.5697e+02,  5.3240e+02,  5.0000e-01],\n",
       "         [-5.0509e+02,  1.5034e+02,  6.1563e+02,  7.6775e+02,  5.0000e-01]]),\n",
       " tensor([9, 5, 6, 0, 1, 2, 3, 4, 7, 8]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmcv.ops import batched_nms\n",
    "boxes = torch.randn(10, 4)*1000\n",
    "scores = torch.randn(10).clamp(min=0.5)\n",
    "idxs = torch.cat([torch.zeros(5), torch.ones(5)]).long()\n",
    "nms_cfg = dict(max_num=10, iou_threshold=0.7)\n",
    "batched_nms(boxes, scores, idxs, nms_cfg, class_agnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "gt_bboxes = torch.Tensor([[ 408.0027,  315.4001,  611.0715,  372.3802],\n",
    "        [ 645.2617,  332.2336,  850.4027,  387.9202],\n",
    "        [ 885.6290,  345.4442, 1093.8781,  402.4242],\n",
    "        [ 346.2424,  328.8053,  366.4457,  422.1526],\n",
    "        [ 398.5853,  388.6484,  601.4492,  445.4065],\n",
    "        [ 637.1537,  403.9441,  840.6491,  461.7406],\n",
    "        [ 876.7070,  416.5464, 1083.5500,  477.1525],\n",
    "        [ 390.0223,  460.6703,  598.7449,  520.6074],\n",
    "        [ 627.0999,  475.6545,  832.0861,  535.1974],\n",
    "        [ 871.8967,  489.6585, 1077.7776,  547.3785],\n",
    "        [ 863.1270,  564.4899, 1066.9784,  629.7786],\n",
    "        [ 209.1294,  639.4113,  306.5333,  675.3246],\n",
    "        [ 833.9768,  591.5006,  854.5640,  608.3724],\n",
    "        [ 797.9386,  602.4206,  813.9556,  626.0806]])\n",
    "scores = torch.randn(gt_bboxes.shape[0])\n",
    "nms_cfg = dict(\n",
    "                type='nms', \n",
    "                iou_threshold=0.5,\n",
    "                max_num=256)\n",
    "idxs = torch.cat([torch.zeros(gt_bboxes.shape[0]//2), torch.ones(gt_bboxes.shape[0]//2)]).long()\n",
    "print(idxs)\n",
    "_, keep = batched_nms(gt_bboxes, scores, idxs, nms_cfg, class_agnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 408.0027,  315.4001,  611.0715,  372.3802],\n",
       "         [ 645.2617,  332.2336,  850.4027,  387.9202],\n",
       "         [ 885.6290,  345.4442, 1093.8781,  402.4242],\n",
       "         [ 346.2424,  328.8053,  366.4457,  422.1526],\n",
       "         [ 398.5853,  388.6484,  601.4492,  445.4065],\n",
       "         [ 637.1537,  403.9441,  840.6491,  461.7406],\n",
       "         [ 876.7070,  416.5464, 1083.5500,  477.1525]]),\n",
       " tensor([[ 390.0223,  460.6703,  598.7449,  520.6074],\n",
       "         [ 627.0999,  475.6545,  832.0861,  535.1974],\n",
       "         [ 871.8967,  489.6585, 1077.7776,  547.3785],\n",
       "         [ 863.1270,  564.4899, 1066.9784,  629.7786],\n",
       "         [ 209.1294,  639.4113,  306.5333,  675.3246],\n",
       "         [ 833.9768,  591.5006,  854.5640,  608.3724],\n",
       "         [ 797.9386,  602.4206,  813.9556,  626.0806]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_bboxes.split(7, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_scores, rank_inds = scores.sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 876.7070,  416.5464, 1083.5500,  477.1525],\n",
       "        [ 408.0027,  315.4001,  611.0715,  372.3802],\n",
       "        [ 398.5853,  388.6484,  601.4492,  445.4065]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_bboxes[rank_inds][:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmdet.core import reduce_mean\n",
    "\n",
    "def get_world_size() -> int:\n",
    "    if not torch.distributed.is_available():\n",
    "        return 1\n",
    "    if not torch.distributed.is_initialized():\n",
    "        return 1\n",
    "    return torch.distributed.get_world_size()\n",
    "\n",
    "def reduce_sum(tensor):\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return tensor\n",
    "    tensor = tensor.clone()\n",
    "    torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
    "    return tensor\n",
    "\n",
    "pos_inds = (torch.randn(10) > 0.5).nonzero().squeeze(-1)\n",
    "num_pos_local = pos_inds.numel()\n",
    "num_gpus = get_world_size()\n",
    "total_num_pos = reduce_sum(pos_inds.new_tensor([num_pos_local])).item()\n",
    "num_pos_avg = max(total_num_pos / num_gpus, 1.0)\n",
    "print(num_pos_avg)\n",
    "\n",
    "print(max(reduce_mean(num_pos_local), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 8, 9])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4193/2733108410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "pos_inds = (torch.randn(10) > 0.5).nonzero().squeeze(-1)\n",
    "print(pos_inds)\n",
    "print(pos_inds.max(dim=1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
